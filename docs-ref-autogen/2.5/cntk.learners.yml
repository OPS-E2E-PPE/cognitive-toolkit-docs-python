### YamlMime:UniversalReference
api_name: []
items:
- children:
  - cntk.learners.Learner
  - cntk.learners.UnitType
  - cntk.learners.UserLearner
  - cntk.learners.adadelta
  - cntk.learners.adagrad
  - cntk.learners.adam
  - cntk.learners.default_unit_gain_value
  - cntk.learners.fsadagrad
  - cntk.learners.learning_parameter_schedule
  - cntk.learners.learning_parameter_schedule_per_sample
  - cntk.learners.learning_rate_schedule
  - cntk.learners.momentum_as_time_constant_schedule
  - cntk.learners.momentum_schedule
  - cntk.learners.momentum_schedule_per_sample
  - cntk.learners.momentum_sgd
  - cntk.learners.nesterov
  - cntk.learners.rmsprop
  - cntk.learners.set_default_unit_gain_value
  - cntk.learners.sgd
  - cntk.learners.training_parameter_schedule
  - cntk.learners.universal
  fullName: cntk.learners
  langs:
  - python
  module: cntk.learners
  name: learners
  source:
    id: learners
    path: bindings/python/cntk\learners\__init__.py
    remote:
      branch: release/2.5
      path: bindings/python/cntk\learners\__init__.py
      repo: https://github.com/Microsoft/CNTK.git
    startLine: 0
  summary: 'A learner tunes a set of parameters during the training process. One can
    use

    different learners for different sets of parameters. Currently, CNTK supports

    the following learning algorithms:



    - @adadelta

    - @adagrad

    - @fsadagrad

    - @adam

    - @momentum_sgd

    - @nesterov

    - @rmsprop

    - @sgd

    - @universal

    '
  type: package
  uid: cntk.learners
- fullName: cntk.learners.adadelta
  langs:
  - python
  module: cntk.learners
  name: adadelta
  source:
    id: adadelta
    path: bindings/python/cntk\internal\swig_helper.py
    remote:
      branch: release/2.5
      path: bindings/python/cntk\internal\swig_helper.py
      repo: https://github.com/Microsoft/CNTK.git
    startLine: 802
  summary: "Creates an AdaDelta learner instance to learn the parameters. See [1]\
    \ for\nmore information.\n\nSee also\n   [1]  Matthew D. Zeiler, [ADADELTA: An\
    \ Adaptive Learning Rate Method](https://arxiv.org/pdf/1212.5701.pdf).\n"
  syntax:
    content: adadelta(parameters, lr, rho, epsilon, l1_regularization_weight=0, l2_regularization_weight=0,
      gaussian_noise_injection_std_dev=0, gradient_clipping_threshold_per_sample=np.inf,
      gradient_clipping_with_truncation=True)
    parameters:
    - description: 'list of network parameters to tune.

        These can be obtained by the root operator''s `parameters`.

        '
      id: parameters
      type:
      - list of parameters
    - description: 'a learning rate in float, or a learning rate schedule.

        See also:  @cntk.learners.learning_parameter_schedule

        '
      id: lr
      type:
      - float, list, output of cntk.learners.learning_parameter_schedule
    - description: 'exponential smooth factor for each minibatch.

        '
      id: rho
      type:
      - float
    - description: 'epsilon for sqrt.

        '
      id: epsilon
      type:
      - float
    - description: 'the L1 regularization weight per sample,

        defaults to 0.0

        '
      id: l1_regularization_weight
      type:
      - float, optional
    - description: 'the L2 regularization weight per sample,

        defaults to 0.0

        '
      id: l2_regularization_weight
      type:
      - float, optional
    - description: 'the standard deviation

        of the Gaussian noise added to parameters post update, defaults to 0.0

        '
      id: gaussian_noise_injection_std_dev
      type:
      - float, optional
    - description: 'clipping threshold

        per sample, defaults to infinity

        '
      id: gradient_clipping_threshold_per_sample
      type:
      - float, optional
    - description: 'use gradient clipping

        with truncation

        '
      id: gradient_clipping_with_truncation
      type:
      - bool, default True
    - description: "use averaged gradient as input to learner.\n\ndeprecated:: 2.2\n\
        \   Use minibatch_size parameter to specify the reference minibatch size.\n"
      id: use_mean_gradient
      type:
      - bool, optional
    - description: 'The minibatch size that the learner''s parameters are designed
        or pre-tuned for. This

        size is usually set to the same as the minibatch data source''s size. CNTK
        will perform automatic scaling of the parameters

        to enable efficient model parameter update implementation while approximate
        the behavior of pre-designed and pre-tuned parameters.

        In case that minibatch_size is not specified, CNTK will inherit the minibatch
        size from the learning rate schedule;

        if the learning rate schedule does not specify the minibatch_size, CNTK will
        set it to @cntk.learners.IGNORE. Setting minibatch_size to @cntk.learners.IGNORE

        will have the learner apply as it is preventing CNTK performing any hyper-parameter
        scaling. See also:  @cntk.learners.learning_parameter_schedule

        '
      id: minibatch_size
      type:
      - int, default None
    - description: 'number of samples as a scheduling unit for learning rate. See
        also:  @cntk.learners.learning_parameter_schedule

        '
      id: epoch_size
      type:
      - optional, int
    return:
      description: 'learner instance that can be passed to

        the @cntk.train.trainer.Trainer

        '
      type:
      - cntk.learners.Learner
  type: function
  uid: cntk.learners.adadelta
- fullName: cntk.learners.adagrad
  langs:
  - python
  module: cntk.learners
  name: adagrad
  seealsoContent: "See also: [1]  J. Duchi, E. Hazan, and Y. Singer. [Adaptive Subgradient\
    \ Methods for Online Learning and Stochastic Optimization](http://www.magicbroom.info/Papers/DuchiHaSi10.pdf).\
    \ The Journal of Machine Learning Research, 2011. \n"
  source:
    id: adagrad
    path: bindings/python/cntk\internal\swig_helper.py
    remote:
      branch: release/2.5
      path: bindings/python/cntk\internal\swig_helper.py
      repo: https://github.com/Microsoft/CNTK.git
    startLine: 870
  summary: 'Creates an AdaGrad learner instance to learn the parameters. See [1] for

    more information.

    '
  syntax:
    content: adagrad(parameters, lr, need_ave_multiplier=True, l1_regularization_weight=0,
      l2_regularization_weight=0, gaussian_noise_injection_std_dev=0, gradient_clipping_threshold_per_sample=np.inf,
      gradient_clipping_with_truncation=True)
    parameters:
    - description: 'list of network parameters to tune.

        These can be obtained by the root operator''s `parameters`.

        '
      id: parameters
      type:
      - list of parameters
    - description: 'a learning rate in float, or a learning rate schedule.

        See also:  @cntk.learners.learning_parameter_schedule

        '
      id: lr
      type:
      - float, list, output of cntk.learners.learning_parameter_schedule
    - description: ''
      id: need_ave_multiplier
      type:
      - bool, default
    - description: 'the L1 regularization weight per sample,

        defaults to 0.0

        '
      id: l1_regularization_weight
      type:
      - float, optional
    - description: 'the L2 regularization weight per sample,

        defaults to 0.0

        '
      id: l2_regularization_weight
      type:
      - float, optional
    - description: 'the standard deviation

        of the Gaussian noise added to parameters post update, defaults to 0.0

        '
      id: gaussian_noise_injection_std_dev
      type:
      - float, optional
    - description: 'clipping threshold

        per sample, defaults to infinity

        '
      id: gradient_clipping_threshold_per_sample
      type:
      - float, optional
    - description: 'use gradient clipping

        with truncation

        '
      id: gradient_clipping_with_truncation
      type:
      - bool, default True
    - description: "use averaged gradient as input to learner.\n\ndeprecated:: 2.2\n\
        \   Use minibatch_size parameter to specify the reference minibatch size.\n"
      id: use_mean_gradient
      type:
      - bool, optional
    - description: 'The minibatch size that the learner''s parameters are designed
        or pre-tuned for. This

        size is usually set to the same as the minibatch data source''s size. CNTK
        will perform automatic scaling of the parameters

        to enable efficient model parameter update implementation while approximate
        the behavior of pre-designed and pre-tuned parameters.

        In case that minibatch_size is not specified, CNTK will inherit the minibatch
        size from the learning rate schedule;

        if the learning rate schedule does not specify the minibatch_size, CNTK will
        set it to @cntk.learners.IGNORE. Setting minibatch_size to @cntk.learners.IGNORE

        will have the learner apply as it is preventing CNTK performing any hyper-parameter
        scaling. See also:  @cntk.learners.learning_parameter_schedule

        '
      id: minibatch_size
      type:
      - int, default None
    - description: 'number of samples as a scheduling unit for learning rate. See
        also:  @cntk.learners.learning_parameter_schedule

        '
      id: epoch_size
      type:
      - optional, int
    return:
      description: 'learner instance that can be passed to

        the @cntk.train.trainer.Trainer

        '
      type:
      - cntk.learners.Learner
  type: function
  uid: cntk.learners.adagrad
- fullName: cntk.learners.adam
  langs:
  - python
  module: cntk.learners
  name: adam
  seealsoContent: "See also: [1] D. Kingma, J. Ba. [Adam: A Method for Stochastic\
    \ Optimization](https://arxiv.org/abs/1412.6980). International Conference for\
    \ Learning Representations, 2015. \n"
  source:
    id: adam
    path: bindings/python/cntk\internal\swig_helper.py
    remote:
      branch: release/2.5
      path: bindings/python/cntk\internal\swig_helper.py
      repo: https://github.com/Microsoft/CNTK.git
    startLine: 1013
  summary: 'Creates an Adam learner instance to learn the parameters. See [1] for
    more

    information.

    '
  syntax:
    content: adam(parameters, lr, momentum, unit_gain=default_unit_gain_value(), variance_momentum=momentum_schedule_per_sample(0.9999986111120757),
      l1_regularization_weight=0, l2_regularization_weight=0, gaussian_noise_injection_std_dev=0,
      gradient_clipping_threshold_per_sample=np.inf, gradient_clipping_with_truncation=True,
      epsilon=1e-8, adamax=False)
    parameters:
    - description: 'list of network parameters to tune.

        These can be obtained by the root operator''s `parameters`.

        '
      id: parameters
      type:
      - list of parameters
    - description: 'a learning rate in float, or a learning rate schedule.

        See also:  @cntk.learners.learning_parameter_schedule

        '
      id: lr
      type:
      - float, list, output of cntk.learners.learning_parameter_schedule
    - description: 'momentum schedule. Note that this is the beta1 parameter in the
        Adam paper [1].

        For additional information, please refer to the [this CNTK Wiki article](https://docs.microsoft.com/en-us/cognitive-toolkit/BrainScript-SGD-Block#converting-learning-rate-and-momentum-parameters-from-other-toolkits).

        '
      id: momentum
      type:
      - float, list, output of cntk.learners.momentum_schedule
    - description: 'when `True`, momentum is interpreted as a unit-gain filter. Defaults

        to the value returned by @cntk.learners.default_unit_gain_value.

        '
      id: unit_gain
    - description: 'variance momentum schedule.

        Note that this is the beta2 parameter in the Adam paper [1]. Defaults to `momentum_schedule_per_sample(0.9999986111120757)`.

        '
      id: variance_momentum
      type:
      - float, list, output of cntk.learners.momentum_schedule
    - description: 'the L1 regularization weight per sample,

        defaults to 0.0

        '
      id: l1_regularization_weight
      type:
      - float, optional
    - description: 'the L2 regularization weight per sample,

        defaults to 0.0

        '
      id: l2_regularization_weight
      type:
      - float, optional
    - description: 'the standard deviation

        of the Gaussian noise added to parameters post update, defaults to 0.0

        '
      id: gaussian_noise_injection_std_dev
      type:
      - float, optional
    - description: 'clipping threshold

        per sample, defaults to infinity

        '
      id: gradient_clipping_threshold_per_sample
      type:
      - float, optional
    - description: 'use gradient clipping

        with truncation

        '
      id: gradient_clipping_with_truncation
      type:
      - bool, default True
    - description: "use averaged gradient as input to learner.\n\ndeprecated:: 2.2\n\
        \   Use minibatch_size parameter to specify the reference minibatch size.\n"
      id: use_mean_gradient
      type:
      - bool, optional
    - description: 'numerical stability constant,

        defaults to 1e-8

        '
      id: epsilon
      type:
      - float, optional
    - description: 'when `True`, use infinity-norm variance momentum update instead
        of L2. Defaults

        to False

        '
      id: adamax
    - description: 'The minibatch size that the learner''s parameters are designed
        or pre-tuned for. This

        size is usually set to the same as the minibatch data source''s size. CNTK
        will perform automatic scaling of the parameters

        to enable efficient model parameter update implementation while approximate
        the behavior of pre-designed and pre-tuned parameters.

        In case that minibatch_size is not specified, CNTK will inherit the minibatch
        size from the learning rate schedule;

        if the learning rate schedule does not specify the minibatch_size, CNTK will
        set it to @cntk.learners.IGNORE. Setting minibatch_size to @cntk.learners.IGNORE

        will have the learner apply as it is preventing CNTK performing any hyper-parameter
        scaling. See also:  @cntk.learners.learning_parameter_schedule

        '
      id: minibatch_size
      type:
      - int, default None
    - description: 'number of samples as a scheduling unit for learning rate, momentum
        and variance_momentum. See also:  @cntk.learners.learning_parameter_schedule

        '
      id: epoch_size
      type:
      - optional, int
    return:
      description: 'learner instance that can be passed to

        the @cntk.train.trainer.Trainer

        '
      type:
      - cntk.learners.Learner
  type: function
  uid: cntk.learners.adam
- fullName: cntk.learners.default_unit_gain_value
  langs:
  - python
  module: cntk.learners
  name: default_unit_gain_value
  source:
    id: default_unit_gain_value
    path: bindings/python/cntk\learners\__init__.py
    remote:
      branch: release/2.5
      path: bindings/python/cntk\learners\__init__.py
      repo: https://github.com/Microsoft/CNTK.git
    startLine: 52
  summary: 'Returns true if by default momentum is applied in the unit-gain fashion.

    '
  syntax:
    content: default_unit_gain_value()
  type: function
  uid: cntk.learners.default_unit_gain_value
- fullName: cntk.learners.fsadagrad
  langs:
  - python
  module: cntk.learners
  name: fsadagrad
  source:
    id: fsadagrad
    path: bindings/python/cntk\internal\swig_helper.py
    remote:
      branch: release/2.5
      path: bindings/python/cntk\internal\swig_helper.py
      repo: https://github.com/Microsoft/CNTK.git
    startLine: 939
  summary: 'Creates an FSAdaGrad learner instance to learn the parameters.

    '
  syntax:
    content: fsadagrad(parameters, lr, momentum, unit_gain=default_unit_gain_value(),
      variance_momentum=momentum_schedule_per_sample(0.9999986111120757), l1_regularization_weight=0,
      l2_regularization_weight=0, gaussian_noise_injection_std_dev=0, gradient_clipping_threshold_per_sample=np.inf,
      gradient_clipping_with_truncation=True)
    parameters:
    - description: 'list of network parameters to tune.

        These can be obtained by the root operator''s `parameters`.

        '
      id: parameters
      type:
      - list of parameters
    - description: 'a learning rate in float, or a learning rate schedule.

        See also:  @cntk.learners.learning_parameter_schedule

        '
      id: lr
      type:
      - float, list, output of cntk.learners.learning_parameter_schedule
    - description: 'momentum schedule.

        For additional information, please refer to the [this CNTK Wiki article](https://docs.microsoft.com/en-us/cognitive-toolkit/BrainScript-SGD-Block#converting-learning-rate-and-momentum-parameters-from-other-toolkits).

        '
      id: momentum
      type:
      - float, list, output of cntk.learners.momentum_schedule
    - description: 'when `True`, momentum is interpreted as a unit-gain filter. Defaults

        to the value returned by @cntk.learners.default_unit_gain_value.

        '
      id: unit_gain
    - description: 'variance momentum schedule. Defaults

        to `momentum_schedule_per_sample(0.9999986111120757)`.

        '
      id: variance_momentum
      type:
      - float, list, output of cntk.learners.momentum_schedule
    - description: 'the L1 regularization weight per sample,

        defaults to 0.0

        '
      id: l1_regularization_weight
      type:
      - float, optional
    - description: 'the L2 regularization weight per sample,

        defaults to 0.0

        '
      id: l2_regularization_weight
      type:
      - float, optional
    - description: 'the standard deviation

        of the Gaussian noise added to parameters post update, defaults to 0.0

        '
      id: gaussian_noise_injection_std_dev
      type:
      - float, optional
    - description: 'clipping threshold

        per sample, defaults to infinity

        '
      id: gradient_clipping_threshold_per_sample
      type:
      - float, optional
    - description: 'use gradient clipping

        with truncation

        '
      id: gradient_clipping_with_truncation
      type:
      - bool, default True
    - description: "use averaged gradient as input to learner.\n\ndeprecated:: 2.2\n\
        \   Use minibatch_size parameter to specify the reference minibatch size.\n"
      id: use_mean_gradient
      type:
      - bool, optional
    - description: 'The minibatch size that the learner''s parameters are designed
        or pre-tuned for. This

        size is usually set to the same as the minibatch data source''s size. CNTK
        will perform automatic scaling of the parameters

        to enable efficient model parameter update implementation while approximate
        the behavior of pre-designed and pre-tuned parameters.

        In case that minibatch_size is not specified, CNTK will inherit the minibatch
        size from the learning rate schedule;

        if the learning rate schedule does not specify the minibatch_size, CNTK will
        set it to @cntk.learners.IGNORE. Setting minibatch_size to @cntk.learners.IGNORE

        will have the learner apply as it is preventing CNTK performing any hyper-parameter
        scaling. See also:  @cntk.learners.learning_parameter_schedule

        '
      id: minibatch_size
      type:
      - int, default None
    - description: 'number of samples as a scheduling unit for learning rate, momentum
        and variance_momentum. See also:  @cntk.learners.learning_parameter_schedule

        '
      id: epoch_size
      type:
      - optional, int
    return:
      description: 'learner instance that can be passed to

        the @cntk.train.trainer.Trainer

        '
      type:
      - cntk.learners.Learner
  type: function
  uid: cntk.learners.fsadagrad
- fullName: cntk.learners.learning_parameter_schedule
  langs:
  - python
  module: cntk.learners
  name: learning_parameter_schedule
  source:
    id: learning_parameter_schedule
    path: bindings/python/cntk\internal\swig_helper.py
    remote:
      branch: release/2.5
      path: bindings/python/cntk\internal\swig_helper.py
      repo: https://github.com/Microsoft/CNTK.git
    startLine: 304
  summary: 'Create a learning parameter schedule.

    '
  syntax:
    content: learning_parameter_schedule(schedule, minibatch_size=None, epoch_size=None)
    parameters:
    - description: 'if float, is the parameter schedule to be used

        for all samples. In case of list [p_1, p_2, .., p_n], the i-th parameter p_i
        in the list is used as the

        value from the (`epoch_size` * (i-1) + 1)-th sample to the (`epoch_size` *
        i)-th sample. If list contains

        pair, i.e. [(num_epoch_1, p_1), (num_epoch_n, p_2), .., (num_epoch_n, p_n)],
        the i-th parameter is used as a

        value from the (`epoch_size` * (num_epoch_0 + ... + num_epoch_2 + ... + num_epoch_(i-1)
        + 1)-th sample to the

        (`epoch_size` * num_epoch_i)-th sample (taking num_epoch_0 = 0 as a special
        initialization).

        '
      id: schedule
      type:
      - float
      - list
    - description: 'an integer to specify the minibatch size that schedule are designed
        for.

        CNTK will scale the schedule internally so as to simulate the behavior of
        the schedule as much as possible

        to match the designed effect. If it is not specified, CNTK will set to the
        special value @cntk.learners.IGNORE.

        '
      id: minibatch_size
      type:
      - int
    - description: 'number of samples as a scheduling unit.

        Parameters in the schedule change their values every `epoch_size`

        samples. If no `epoch_size` is provided, this parameter is substituted

        by the size of the full data sweep, in which case the scheduling unit is

        the entire data sweep (as indicated by the MinibatchSource) and parameters

        change their values on the sweep-by-sweep basis specified by the

        `schedule`.

        '
      id: epoch_size
      type:
      - optional, int
    return:
      description: 'learning parameter schedule

        '
  type: function
  uid: cntk.learners.learning_parameter_schedule
- fullName: cntk.learners.learning_parameter_schedule_per_sample
  langs:
  - python
  module: cntk.learners
  name: learning_parameter_schedule_per_sample
  source:
    id: learning_parameter_schedule_per_sample
    path: bindings/python/cntk\internal\swig_helper.py
    remote:
      branch: release/2.5
      path: bindings/python/cntk\internal\swig_helper.py
      repo: https://github.com/Microsoft/CNTK.git
    startLine: 357
  summary: 'Create a learning parameter schedule as if the parameter is applied to
    minibatches of size 1. CNTK

    will scale the parameters accordingly with respect to the actual minibatch size.

    '
  syntax:
    content: learning_parameter_schedule_per_sample(schedule, epoch_size=None)
    parameters:
    - description: 'if float, is the parameter schedule to be used

        for all samples. In case of list [p_1, p_2, .., p_n], the i-th parameter p_i
        in the list is used as the

        value from the (`epoch_size` * (i-1) + 1)-th sample to the (`epoch_size` *
        i)-th sample. If list contains

        pair, i.e. [(num_epoch_1, p_1), (num_epoch_n, p_2), .., (num_epoch_n, p_n)],
        the i-th parameter is used as a

        value from the (`epoch_size` * (num_epoch_0 + ... + num_epoch_2 + ... + num_epoch_(i-1)
        + 1)-th sample to the

        (`epoch_size` * num_epoch_i)-th sample (taking num_epoch_0 = 0 as a special
        initialization).

        '
      id: schedule
      type:
      - float
      - list
    - description: 'number of samples as a scheduling unit.

        Parameters in the schedule change their values every `epoch_size`

        samples. If no `epoch_size` is provided, this parameter is substituted

        by the size of the full data sweep, in which case the scheduling unit is

        the entire data sweep (as indicated by the MinibatchSource) and parameters

        change their values on the sweep-by-sweep basis specified by the

        `schedule`.

        '
      id: epoch_size
      type:
      - optional, int
    return:
      description: 'learning parameter schedule as if it is applied to minibatches
        of size 1.

        '
  type: function
  uid: cntk.learners.learning_parameter_schedule_per_sample
- fullName: cntk.learners.learning_rate_schedule
  langs:
  - python
  module: cntk.learners
  name: learning_rate_schedule
  seealsoContent: "See also: @cntk.learners.training_parameter_schedule \n"
  source:
    id: learning_rate_schedule
    path: bindings/python/cntk\internal\swig_helper.py
    remote:
      branch: release/2.5
      path: bindings/python/cntk\internal\swig_helper.py
      repo: https://github.com/Microsoft/CNTK.git
    startLine: 384
  summary: 'Deprecated:: 2.2


    Create a learning rate schedule (using the same semantics as

    @cntk.learners.training_parameter_schedule).

    '
  syntax:
    content: learning_rate_schedule(lr, unit, epoch_size=None)
    parameters:
    - description: 'see parameter `schedule` in

        @cntk.learners.training_parameter_schedule.

        '
      id: lr
      type:
      - float
      - list
    - description: "see parameter\n`unit` in @cntk.learners.training_parameter_schedule.\n\
        \n   deprecated:: 2.2\n      Use minibatch_size parameter to specify the reference\
        \ minbiatch size instead.\n"
      id: unit
      type:
      - cntk.learners.UnitType
    - description: 'see parameter `epoch_size` in

        @cntk.learners.training_parameter_schedule.

        '
      id: epoch_size
      type:
      - int
    return:
      description: 'learning rate schedule

        '
  type: function
  uid: cntk.learners.learning_rate_schedule
- example:
  - '

    ```


    >>> # Use a fixed momentum of 1100 for all samples

    >>> m = momentum_as_time_constant_schedule(1100)

    ```



    ```


    >>> # Use the time constant 1100 for the first 1000 samples,

    >>> # then 1500 for the remaining ones

    >>> m = momentum_as_time_constant_schedule([1100, 1500], 1000)

    ```

    '
  fullName: cntk.learners.momentum_as_time_constant_schedule
  langs:
  - python
  module: cntk.learners
  name: momentum_as_time_constant_schedule
  source:
    id: momentum_as_time_constant_schedule
    path: bindings/python/cntk\internal\swig_helper.py
    remote:
      branch: release/2.5
      path: bindings/python/cntk\internal\swig_helper.py
      repo: https://github.com/Microsoft/CNTK.git
    startLine: 470
  summary: "Create a momentum schedule in a minibatch-size agnostic way\n(using the\
    \ same semantics as @cntk.learners.training_parameter_schedule\nwith *unit=UnitType.sample*).\n\
    \nDeprecated:: 2.2\n   This is for legacy API.\n   In this legacy API,:\n\n  \
    \ <!-- literal_block {\"names\": [], \"xml:space\": \"preserve\", \"backrefs\"\
    : [], \"classes\": [], \"ids\": [], \"dupnames\": []} -->\n\n   ````\n\n     \
    \ #assume the desired minibatch size invariant constant momentum rate is: momentum_rate\n\
    \      momentum_time_constant = -minibatch_size/np.log(momentum_rate)\n      momentum\
    \ = momentum_as_time_constant_schedule(momentum_time_constant)\n      ````\n\n\
    \   The equivalent code in the latest API,\n\n   <!-- literal_block {\"names\"\
    : [], \"xml:space\": \"preserve\", \"backrefs\": [], \"classes\": [], \"ids\"\
    : [], \"dupnames\": []} -->\n\n   ````\n\n      momentum = momentum_schedule(momentum_rate,\
    \ minibatch_size = minibatch_size)\n      ````\n\nCNTK specifies momentum in a\
    \ minibatch-size agnostic way as the time\nconstant (in samples) of a unit-gain\
    \ 1st-order IIR filter. The value\nspecifies the number of samples after which\
    \ a gradient has an effect of\n1/e=37%.\n\nIf you want to specify the momentum\
    \ per N samples (or per minibatch),\nuse @cntk.learners.momentum_schedule.\n"
  syntax:
    content: momentum_as_time_constant_schedule(momentum, epoch_size=None)
    return:
      description: 'momentum as time constant schedule

        '
  type: function
  uid: cntk.learners.momentum_as_time_constant_schedule
- example:
  - '

    ```


    >>> # Use a fixed momentum of 0.99 for all samples

    >>> m = momentum_schedule(0.99)

    ```



    ```


    >>> # Use the momentum value 0.99 for the first 1000 samples,

    >>> # then 0.9 for the remaining ones

    >>> m = momentum_schedule([0.99,0.9], 1000)

    >>> m[0], m[999], m[1000], m[1001]

    (0.99, 0.99, 0.9, 0.9)

    ```



    ```


    >>> # Use the momentum value 0.99 for the first 999 samples,

    >>> # then 0.88 for the next 888 samples, and 0.77 for the

    >>> # the remaining ones

    >>> m = momentum_schedule([(999,0.99),(888,0.88),(0, 0.77)])

    >>> m[0], m[998], m[999], m[999+888-1], m[999+888]

    (0.99, 0.99, 0.88, 0.88, 0.77)

    ```

    '
  fullName: cntk.learners.momentum_schedule
  langs:
  - python
  module: cntk.learners
  name: momentum_schedule
  source:
    id: momentum_schedule
    path: bindings/python/cntk\internal\swig_helper.py
    remote:
      branch: release/2.5
      path: bindings/python/cntk\internal\swig_helper.py
      repo: https://github.com/Microsoft/CNTK.git
    startLine: 412
  summary: 'Create a momentum schedule (using the same semantics as

    @cntk.learners.learning_parameter_schedule) which applies the momentum

    decay every N samples where N is specified by the argument *minibatch_size*.

    '
  syntax:
    content: momentum_schedule(momentum, epoch_size=None, minibatch_size=None)
    return:
      description: 'momentum schedule

        '
  type: function
  uid: cntk.learners.momentum_schedule
- fullName: cntk.learners.momentum_schedule_per_sample
  langs:
  - python
  module: cntk.learners
  name: momentum_schedule_per_sample
  source:
    id: momentum_schedule_per_sample
    path: bindings/python/cntk\internal\swig_helper.py
    remote:
      branch: release/2.5
      path: bindings/python/cntk\internal\swig_helper.py
      repo: https://github.com/Microsoft/CNTK.git
    startLine: 453
  summary: 'Create a per-sample momentum schedule (using the same semantics as

    @cntk.learners.momentum_schedule but specializing in per sample momentum schedule).

    '
  syntax:
    content: momentum_schedule_per_sample(momentum, epoch_size=None)
    parameters:
    - description: 'see parameter `schedule` in

        @cntk.learners.training_parameter_schedule.

        '
      id: momentum
      type:
      - float
      - list
    - description: 'see parameter `epoch_size` in

        @cntk.learners.momentum_schedule.

        '
      id: epoch_size
      type:
      - int
    return:
      description: 'momentum schedule

        '
  type: function
  uid: cntk.learners.momentum_schedule_per_sample
- fullName: cntk.learners.momentum_sgd
  langs:
  - python
  module: cntk.learners
  name: momentum_sgd
  source:
    id: momentum_sgd
    path: bindings/python/cntk\internal\swig_helper.py
    remote:
      branch: release/2.5
      path: bindings/python/cntk\internal\swig_helper.py
      repo: https://github.com/Microsoft/CNTK.git
    startLine: 660
  summary: 'Creates a Momentum SGD learner instance to learn the parameters.

    '
  syntax:
    content: momentum_sgd(parameters, lr, momentum, unit_gain=default_unit_gain_value(),
      l1_regularization_weight=0.0, l2_regularization_weight=0, gaussian_noise_injection_std_dev=0,
      gradient_clipping_threshold_per_sample=np.inf, gradient_clipping_with_truncation=True)
    parameters:
    - description: 'list of network parameters to tune.

        These can be obtained by the root operator''s `parameters`.

        '
      id: parameters
      type:
      - list of parameters
    - description: 'a learning rate in float, or a learning rate schedule.

        See also:  @cntk.learners.learning_parameter_schedule

        '
      id: lr
      type:
      - float, list, output of cntk.learners.learning_parameter_schedule
    - description: 'momentum schedule.

        For additional information, please refer to the [this CNTK Wiki article](https://docs.microsoft.com/en-us/cognitive-toolkit/BrainScript-SGD-Block#converting-learning-rate-and-momentum-parameters-from-other-toolkits).

        '
      id: momentum
      type:
      - float, list, output of cntk.learners.momentum_schedule
    - description: 'when `True`, momentum is interpreted as a unit-gain filter. Defaults

        to the value returned by @cntk.learners.default_unit_gain_value.

        '
      id: unit_gain
    - description: 'the L1 regularization weight per sample,

        defaults to 0.0

        '
      id: l1_regularization_weight
      type:
      - float, optional
    - description: 'the L2 regularization weight per sample,

        defaults to 0.0

        '
      id: l2_regularization_weight
      type:
      - float, optional
    - description: 'the standard deviation

        of the Gaussian noise added to parameters post update, defaults to 0.0

        '
      id: gaussian_noise_injection_std_dev
      type:
      - float, optional
    - description: 'clipping threshold

        per sample, defaults to infinity

        '
      id: gradient_clipping_threshold_per_sample
      type:
      - float, optional
    - description: 'use gradient clipping

        with truncation

        '
      id: gradient_clipping_with_truncation
      type:
      - bool, default True
    - description: "use averaged gradient as input to learner.\n\ndeprecated:: 2.2\n\
        \   Use minibatch_size parameter to specify the reference minibatch size.\n"
      id: use_mean_gradient
      type:
      - bool, optional
    - description: 'The minibatch size that the learner''s parameters are designed
        or pre-tuned for. This

        size is usually set to the same as the minibatch data source''s size. CNTK
        will perform automatic scaling of the parameters

        to enable efficient model parameter update implementation while approximate
        the behavior of pre-designed and pre-tuned parameters.

        In case that minibatch_size is not specified, CNTK will inherit the minibatch
        size from the learning rate schedule;

        if the learning rate schedule does not specify the minibatch_size, CNTK will
        set it to @cntk.learners.IGNORE. Setting minibatch_size to @cntk.learners.IGNORE

        will have the learner apply as it is preventing CNTK performing any hyper-parameter
        scaling. See also:  @cntk.learners.learning_parameter_schedule

        '
      id: minibatch_size
      type:
      - int, default None
    - description: 'number of samples as a scheduling unit for learning rate and momentum.
        See also:  @cntk.learners.learning_parameter_schedule

        '
      id: epoch_size
      type:
      - optional, int
    return:
      description: 'learner instance that can be passed to

        the @cntk.train.trainer.Trainer

        '
      type:
      - cntk.learners.Learner
  type: function
  uid: cntk.learners.momentum_sgd
- fullName: cntk.learners.nesterov
  langs:
  - python
  module: cntk.learners
  name: nesterov
  seealsoContent: "See also: [1] Y. Nesterov. A Method of Solving a Convex Programming\
    \ Problem with Convergence Rate O(1/ sqrt(k)). Soviet Mathematics Doklady, 1983.\
    \ \n\n  [2] I. Sutskever, J. Martens, G. Dahl, and G. Hinton. [On the\n  Importance\
    \ of Initialization and Momentum in Deep Learning](http://www.cs.toronto.edu/~fritz/absps/momentum.pdf).\
    \  Proceedings\n  of the 30th International Conference on Machine Learning, 2013.\n"
  source:
    id: nesterov
    path: bindings/python/cntk\internal\swig_helper.py
    remote:
      branch: release/2.5
      path: bindings/python/cntk\internal\swig_helper.py
      repo: https://github.com/Microsoft/CNTK.git
    startLine: 726
  summary: 'Creates a Nesterov SGD learner instance to learn the parameters. This
    was

    originally proposed by Nesterov [1] in 1983 and then shown to work well in

    a deep learning context by Sutskever, et al. [2].

    '
  syntax:
    content: nesterov(parameters, lr, momentum, unit_gain=default_unit_gain_value(),
      l1_regularization_weight=0, l2_regularization_weight=0, gaussian_noise_injection_std_dev=0,
      gradient_clipping_threshold_per_sample=np.inf, gradient_clipping_with_truncation=True)
    parameters:
    - description: 'list of network parameters to tune.

        These can be obtained by the root operator''s `parameters`.

        '
      id: parameters
      type:
      - list of parameters
    - description: 'a learning rate in float, or a learning rate schedule.

        See also:  @cntk.learners.learning_parameter_schedule

        '
      id: lr
      type:
      - float, list, output of cntk.learners.learning_parameter_schedule
    - description: 'momentum schedule.

        For additional information, please refer to the [this CNTK Wiki article](https://docs.microsoft.com/en-us/cognitive-toolkit/BrainScript-SGD-Block#converting-learning-rate-and-momentum-parameters-from-other-toolkits).

        '
      id: momentum
      type:
      - float, list, output of cntk.learners.momentum_schedule
    - description: 'when `True`, momentum is interpreted as a unit-gain filter. Defaults

        to the value returned by @cntk.learners.default_unit_gain_value.

        '
      id: unit_gain
    - description: 'the L1 regularization weight per sample,

        defaults to 0.0

        '
      id: l1_regularization_weight
      type:
      - float, optional
    - description: 'the L2 regularization weight per sample,

        defaults to 0.0

        '
      id: l2_regularization_weight
      type:
      - float, optional
    - description: 'the standard deviation

        of the Gaussian noise added to parameters post update, defaults to 0.0

        '
      id: gaussian_noise_injection_std_dev
      type:
      - float, optional
    - description: 'clipping threshold

        per sample, defaults to infinity

        '
      id: gradient_clipping_threshold_per_sample
      type:
      - float, optional
    - description: 'use gradient clipping

        with truncation

        '
      id: gradient_clipping_with_truncation
      type:
      - bool, default True
    - description: "use averaged gradient as input to learner.\n\ndeprecated:: 2.2\n\
        \   Use minibatch_size parameter to specify the reference minibatch size.\n"
      id: use_mean_gradient
      type:
      - bool, optional
    - description: 'The minibatch size that the learner''s parameters are designed
        or pre-tuned for. This

        size is usually set to the same as the minibatch data source''s size. CNTK
        will perform automatic scaling of the parameters

        to enable efficient model parameter update implementation while approximate
        the behavior of pre-designed and pre-tuned parameters.

        In case that minibatch_size is not specified, CNTK will inherit the minibatch
        size from the learning rate schedule;

        if the learning rate schedule does not specify the minibatch_size, CNTK will
        set it to @cntk.learners.IGNORE. Setting minibatch_size to @cntk.learners.IGNORE

        will have the learner apply as it is preventing CNTK performing any hyper-parameter
        scaling. See also:  @cntk.learners.learning_parameter_schedule

        '
      id: minibatch_size
      type:
      - int, default None
    - description: 'number of samples as a scheduling unit for learning rate and momentum.
        See also:  @cntk.learners.learning_parameter_schedule

        '
      id: epoch_size
      type:
      - optional, int
    return:
      description: 'learner instance that can be passed to

        the @cntk.train.trainer.Trainer

        '
      type:
      - cntk.learners.Learner
  type: function
  uid: cntk.learners.nesterov
- fullName: cntk.learners.rmsprop
  langs:
  - python
  module: cntk.learners
  name: rmsprop
  source:
    id: rmsprop
    path: bindings/python/cntk\internal\swig_helper.py
    remote:
      branch: release/2.5
      path: bindings/python/cntk\internal\swig_helper.py
      repo: https://github.com/Microsoft/CNTK.git
    startLine: 1095
  summary: 'Creates an RMSProp learner instance to learn the parameters.

    '
  syntax:
    content: rmsprop(parameters, lr, gamma, inc, dec, max, min, need_ave_multiplier=True,
      l1_regularization_weight=0, l2_regularization_weight=0, gaussian_noise_injection_std_dev=0,
      gradient_clipping_threshold_per_sample=np.inf, gradient_clipping_with_truncation=True)
    parameters:
    - description: 'list of network parameters to tune.

        These can be obtained by the root operator''s `parameters`.

        '
      id: parameters
      type:
      - list of parameters
    - description: 'a learning rate in float, or a learning rate schedule.

        See also:  @cntk.learners.learning_parameter_schedule

        '
      id: lr
      type:
      - float, list, output of cntk.learners.learning_parameter_schedule
    - description: 'Trade-off factor for current and previous gradients. Common value
        is 0.95. Should be in range (0.0, 1.0)

        '
      id: gamma
      type:
      - float
    - description: 'Increasing factor when trying to adjust current learning_rate.
        Should be greater than 1

        '
      id: inc
      type:
      - float
    - description: 'Decreasing factor when trying to adjust current learning_rate.
        Should be in range (0.0, 1.0)

        '
      id: dec
      type:
      - float
    - description: 'Maximum scale allowed for the initial learning_rate. Should be
        greater than zero and min

        '
      id: max
      type:
      - float
    - description: 'Minimum scale allowed for the initial learning_rate. Should be
        greater than zero

        '
      id: min
      type:
      - float
    - description: ''
      id: need_ave_multiplier
      type:
      - bool, default True
    - description: 'the L1 regularization weight per sample,

        defaults to 0.0

        '
      id: l1_regularization_weight
      type:
      - float, optional
    - description: 'the L2 regularization weight per sample,

        defaults to 0.0

        '
      id: l2_regularization_weight
      type:
      - float, optional
    - description: 'the standard deviation

        of the Gaussian noise added to parameters post update, defaults to 0.0

        '
      id: gaussian_noise_injection_std_dev
      type:
      - float, optional
    - description: 'clipping threshold

        per sample, defaults to infinity

        '
      id: gradient_clipping_threshold_per_sample
      type:
      - float, optional
    - description: 'use gradient clipping

        with truncation

        '
      id: gradient_clipping_with_truncation
      type:
      - bool, default True
    - description: "use averaged gradient as input to learner.\n\ndeprecated:: 2.2\n\
        \   Use minibatch_size parameter to specify the reference minibatch size.\n"
      id: use_mean_gradient
      type:
      - bool, optional
    - description: 'The minibatch size that the learner''s parameters are designed
        or pre-tuned for. This

        size is usually set to the same as the minibatch data source''s size. CNTK
        will perform automatic scaling of the parameters

        to enable efficient model parameter update implementation while approximate
        the behavior of pre-designed and pre-tuned parameters.

        In case that minibatch_size is not specified, CNTK will inherit the minibatch
        size from the learning rate schedule;

        if the learning rate schedule does not specify the minibatch_size, CNTK will
        set it to @cntk.learners.IGNORE. Setting minibatch_size to @cntk.learners.IGNORE

        will have the learner apply as it is preventing CNTK performing any hyper-parameter
        scaling. See also:  @cntk.learners.learning_parameter_schedule

        '
      id: minibatch_size
      type:
      - int, default None
    - description: 'number of samples as a scheduling unit for learning rate. See
        also:  @cntk.learners.learning_parameter_schedule

        '
      id: epoch_size
      type:
      - optional, int
    return:
      description: 'learner instance that can be passed to

        the @cntk.train.trainer.Trainer

        '
      type:
      - cntk.learners.Learner
  type: function
  uid: cntk.learners.rmsprop
- fullName: cntk.learners.set_default_unit_gain_value
  langs:
  - python
  module: cntk.learners
  name: set_default_unit_gain_value
  source:
    id: set_default_unit_gain_value
    path: bindings/python/cntk\learners\__init__.py
    remote:
      branch: release/2.5
      path: bindings/python/cntk\learners\__init__.py
      repo: https://github.com/Microsoft/CNTK.git
    startLine: 59
  summary: 'Sets globally default unit-gain flag value.

    '
  syntax:
    content: set_default_unit_gain_value(value)
    parameters:
    - id: value
  type: function
  uid: cntk.learners.set_default_unit_gain_value
- fullName: cntk.learners.sgd
  langs:
  - python
  module: cntk.learners
  name: sgd
  seealsoContent: "See also: [1] L. Bottou. [Stochastic Gradient Descent Tricks](https://www.microsoft.com/en-us/research/publication/stochastic-gradient-tricks).\
    \ Neural Networks: Tricks of the Trade: Springer, 2012. \n"
  source:
    id: sgd
    path: bindings/python/cntk\internal\swig_helper.py
    remote:
      branch: release/2.5
      path: bindings/python/cntk\internal\swig_helper.py
      repo: https://github.com/Microsoft/CNTK.git
    startLine: 593
  summary: 'Creates an SGD learner instance to learn the parameters. See [1] for more

    information on how to set the parameters.

    '
  syntax:
    content: sgd(parameters, lr, l1_regularization_weight=0, l2_regularization_weight=0,
      gaussian_noise_injection_std_dev=0, gradient_clipping_threshold_per_sample=np.inf,
      gradient_clipping_with_truncation=True)
    parameters:
    - description: 'list of network parameters to tune.

        These can be obtained by the ''.parameters()'' method of the root

        operator.

        '
      id: parameters
      type:
      - list of parameters
    - description: 'a learning rate in float, or a learning rate schedule.

        See also:  @cntk.learners.learning_parameter_schedule

        '
      id: lr
      type:
      - float, list, output of cntk.learners.learning_parameter_schedule
    - description: 'the L1 regularization weight per sample,

        defaults to 0.0

        '
      id: l1_regularization_weight
      type:
      - float, optional
    - description: 'the L2 regularization weight per sample,

        defaults to 0.0

        '
      id: l2_regularization_weight
      type:
      - float, optional
    - description: 'the standard deviation

        of the Gaussian noise added to parameters post update, defaults to 0.0

        '
      id: gaussian_noise_injection_std_dev
      type:
      - float, optional
    - description: 'clipping threshold

        per sample, defaults to infinity

        '
      id: gradient_clipping_threshold_per_sample
      type:
      - float, optional
    - description: 'use gradient clipping

        with truncation

        '
      id: gradient_clipping_with_truncation
      type:
      - bool, default True
    - description: "use averaged gradient as input to learner.\n\ndeprecated:: 2.2\n\
        \   Use minibatch_size parameter to specify the reference minibatch size.\n"
      id: use_mean_gradient
      type:
      - bool, optional
    - description: 'The minibatch size that the learner''s parameters are designed
        or pre-tuned for. This

        size is usually set to the same as the minibatch data source''s size. CNTK
        will perform automatic scaling of the parameters

        to enable efficient model parameter update implementation while approximate
        the behavior of pre-designed and pre-tuned parameters.

        In case that minibatch_size is not specified, CNTK will inherit the minibatch
        size from the learning rate schedule;

        if the learning rate schedule does not specify the minibatch_size, CNTK will
        set it to @cntk.learners.IGNORE. Setting minibatch_size to @cntk.learners.IGNORE

        will have the learner apply as it is preventing CNTK performing any hyper-parameter
        scaling. See also:  @cntk.learners.learning_parameter_schedule

        '
      id: minibatch_size
      type:
      - int, default None
    - description: 'number of samples as a scheduling unit for learning rate. See
        also:  @cntk.learners.learning_parameter_schedule

        '
      id: epoch_size
      type:
      - optional, int
    return:
      description: 'learner instance that can be passed to

        the @cntk.train.trainer.Trainer

        '
      type:
      - cntk.learners.Learner
  type: function
  uid: cntk.learners.sgd
- example:
  - '

    ```


    >>> # Use a fixed value 0.01 for all samples

    >>> s = training_parameter_schedule(0.01)

    >>> s[0], s[1]

    (0.01, 0.01)

    ```



    ```


    >>> # Use 0.01 for the first 1000 samples, then 0.001 for the remaining ones

    >>> s = training_parameter_schedule([0.01, 0.001], epoch_size=1000)

    >>> s[0], s[1], s[1000], s[1001]

    (0.01, 0.01, 0.001, 0.001)

    ```



    ```


    >>> # Use 0.1 for the first 12 epochs, then 0.01 for the next 15,

    >>> # followed by 0.001 for the remaining ones, with a 100 samples in an epoch

    >>> s = training_parameter_schedule([(12, 0.1), (15, 0.01), (1, 0.001)], epoch_size=100)

    >>> s[0], s[1199], s[1200], s[2699], s[2700], s[5000]

    (0.1, 0.1, 0.01, 0.01, 0.001, 0.001)

    ```

    '
  fullName: cntk.learners.training_parameter_schedule
  langs:
  - python
  module: cntk.learners
  name: training_parameter_schedule
  seealsoContent: "See also: @cntk.learners.learning_parameter_schedule \n"
  source:
    id: training_parameter_schedule
    path: bindings/python/cntk\internal\swig_helper.py
    remote:
      branch: release/2.5
      path: bindings/python/cntk\internal\swig_helper.py
      repo: https://github.com/Microsoft/CNTK.git
    startLine: 225
  summary: 'Deprecated:: 2.2


    Create a training parameter schedule containing either per-sample (default)

    or per-minibatch values.

    '
  syntax:
    content: 'training_parameter_schedule(schedule, unit=<UnitType.minibatch: ''minibatch''>,
      epoch_size=None)'
    parameters:
    - description: 'if float, is the parameter schedule to be used

        for all samples. In case of list, the elements are used as the

        values for `epoch_size` samples. If list contains pair, the second element
        is

        used as a value for (`epoch_size` x first element) samples

        '
      id: schedule
      type:
      - float
      - list
    - description: "one of two\n* `sample`: the returned schedule contains per-sample\
        \ values\n* `minibatch`: the returned schedule contains per-minibatch values.\n\
        \n   deprecated:: 2.2\n      Use minibatch_size parameter to specify the reference\
        \ minbiatch size.\n"
      id: unit
      type:
      - cntk.learners.UnitType
    - description: 'number of samples as a scheduling unit.

        Parameters in the schedule change their values every `epoch_size`

        samples. If no `epoch_size` is provided, this parameter is substituted

        by the size of the full data sweep, in which case the scheduling unit is

        the entire data sweep (as indicated by the MinibatchSource) and parameters

        change their values on the sweep-by-sweep basis specified by the

        `schedule`.

        '
      id: epoch_size
      type:
      - optional, int
    return:
      description: 'training parameter schedule

        '
  type: function
  uid: cntk.learners.training_parameter_schedule
- example:
  - '

    ```


    >>> def my_adagrad(parameters, gradients):

    ...     accumulators = [C.constant(0, shape=p.shape, dtype=p.dtype, name=''accum'')
    for p in parameters]

    ...     update_funcs = []

    ...     for p, g, a in zip(parameters, gradients, accumulators):

    ...         accum_new = C.assign(a, g * g)

    ...         update_funcs.append(C.assign(p, p - 0.01 * g / C.sqrt(accum_new +
    1e-6)))

    ...     return C.combine(update_funcs)

    ...

    >>> x = C.input_variable((10,))

    >>> y = C.input_variable((2,))

    >>> z = C.layers.Sequential([C.layers.Dense(100, activation=C.relu), C.layers.Dense(2)])(x)

    >>> loss = C.cross_entropy_with_softmax(z, y)

    >>> learner = C.universal(my_adagrad, z.parameters)

    >>> trainer = C.Trainer(z, loss, learner)

    >>> # now trainer can be used as any other Trainer

    ```

    '
  fullName: cntk.learners.universal
  langs:
  - python
  module: cntk.learners
  name: universal
  source:
    id: universal
    path: bindings/python/cntk\internal\swig_helper.py
    remote:
      branch: release/2.5
      path: bindings/python/cntk\internal\swig_helper.py
      repo: https://github.com/Microsoft/CNTK.git
    startLine: 1165
  summary: 'Creates a learner which uses a CNTK function to update the parameters.

    '
  syntax:
    content: universal(update_func, parameters)
    parameters:
    - description: 'function that takes parameters and gradients as arguments and

        returns a @cntk.ops.functions.Function that performs the

        desired updates. The returned function updates the parameters by

        means of containing @cntk.ops.assign operations.

        If `update_func` does not contain @cntk.ops.assign operations

        the parameters will not be updated.

        '
      id: update_func
    - description: 'list of network parameters to tune.

        These can be obtained by the root operator''s *parameters*.

        '
      id: parameters
      type:
      - list
    return:
      description: 'learner instance that can be passed to

        the @cntk.train.trainer.Trainer

        '
      type:
      - cntk.learners.Learner
  type: function
  uid: cntk.learners.universal
references:
- fullName: cntk.learners.Learner
  isExternal: false
  name: Learner
  parent: cntk.learners
  uid: cntk.learners.Learner
- fullName: cntk.learners.UnitType
  isExternal: false
  name: UnitType
  parent: cntk.learners
  uid: cntk.learners.UnitType
- fullName: cntk.learners.UserLearner
  isExternal: false
  name: UserLearner
  parent: cntk.learners
  uid: cntk.learners.UserLearner
- fullName: cntk.learners.adadelta
  isExternal: false
  name: adadelta
  parent: cntk.learners
  uid: cntk.learners.adadelta
- fullName: cntk.learners.adagrad
  isExternal: false
  name: adagrad
  parent: cntk.learners
  uid: cntk.learners.adagrad
- fullName: cntk.learners.adam
  isExternal: false
  name: adam
  parent: cntk.learners
  uid: cntk.learners.adam
- fullName: cntk.learners.default_unit_gain_value
  isExternal: false
  name: default_unit_gain_value
  parent: cntk.learners
  uid: cntk.learners.default_unit_gain_value
- fullName: cntk.learners.fsadagrad
  isExternal: false
  name: fsadagrad
  parent: cntk.learners
  uid: cntk.learners.fsadagrad
- fullName: cntk.learners.learning_parameter_schedule
  isExternal: false
  name: learning_parameter_schedule
  parent: cntk.learners
  uid: cntk.learners.learning_parameter_schedule
- fullName: cntk.learners.learning_parameter_schedule_per_sample
  isExternal: false
  name: learning_parameter_schedule_per_sample
  parent: cntk.learners
  uid: cntk.learners.learning_parameter_schedule_per_sample
- fullName: cntk.learners.learning_rate_schedule
  isExternal: false
  name: learning_rate_schedule
  parent: cntk.learners
  uid: cntk.learners.learning_rate_schedule
- fullName: cntk.learners.momentum_as_time_constant_schedule
  isExternal: false
  name: momentum_as_time_constant_schedule
  parent: cntk.learners
  uid: cntk.learners.momentum_as_time_constant_schedule
- fullName: cntk.learners.momentum_schedule
  isExternal: false
  name: momentum_schedule
  parent: cntk.learners
  uid: cntk.learners.momentum_schedule
- fullName: cntk.learners.momentum_schedule_per_sample
  isExternal: false
  name: momentum_schedule_per_sample
  parent: cntk.learners
  uid: cntk.learners.momentum_schedule_per_sample
- fullName: cntk.learners.momentum_sgd
  isExternal: false
  name: momentum_sgd
  parent: cntk.learners
  uid: cntk.learners.momentum_sgd
- fullName: cntk.learners.nesterov
  isExternal: false
  name: nesterov
  parent: cntk.learners
  uid: cntk.learners.nesterov
- fullName: cntk.learners.rmsprop
  isExternal: false
  name: rmsprop
  parent: cntk.learners
  uid: cntk.learners.rmsprop
- fullName: cntk.learners.set_default_unit_gain_value
  isExternal: false
  name: set_default_unit_gain_value
  parent: cntk.learners
  uid: cntk.learners.set_default_unit_gain_value
- fullName: cntk.learners.sgd
  isExternal: false
  name: sgd
  parent: cntk.learners
  uid: cntk.learners.sgd
- fullName: cntk.learners.training_parameter_schedule
  isExternal: false
  name: training_parameter_schedule
  parent: cntk.learners
  uid: cntk.learners.training_parameter_schedule
- fullName: cntk.learners.universal
  isExternal: false
  name: universal
  parent: cntk.learners
  uid: cntk.learners.universal
- fullName: float, list, output of cntk.learners.learning_parameter_schedule
  name: float, list, learning_parameter_schedule
  spec.python:
  - fullName: float
    name: float
    uid: float
  - fullName: ', '
    name: ', '
  - fullName: list
    name: list
    uid: list
  - fullName: ', '
    name: ', '
  - fullName: output of cntk.learners.learning_parameter_schedule
    name: learning_parameter_schedule
    uid: output of cntk.learners.learning_parameter_schedule
  uid: float, list, output of cntk.learners.learning_parameter_schedule
- fullName: float, optional
  name: float, optional
  spec.python:
  - fullName: float
    name: float
    uid: float
  - fullName: ', '
    name: ', '
  - fullName: optional
    name: optional
    uid: optional
  uid: float, optional
- fullName: float, optional
  name: float, optional
  spec.python:
  - fullName: float
    name: float
    uid: float
  - fullName: ', '
    name: ', '
  - fullName: optional
    name: optional
    uid: optional
  uid: float, optional
- fullName: float, optional
  name: float, optional
  spec.python:
  - fullName: float
    name: float
    uid: float
  - fullName: ', '
    name: ', '
  - fullName: optional
    name: optional
    uid: optional
  uid: float, optional
- fullName: float, optional
  name: float, optional
  spec.python:
  - fullName: float
    name: float
    uid: float
  - fullName: ', '
    name: ', '
  - fullName: optional
    name: optional
    uid: optional
  uid: float, optional
- fullName: bool, default True
  name: bool, default True
  spec.python:
  - fullName: bool
    name: bool
    uid: bool
  - fullName: ', '
    name: ', '
  - fullName: default True
    name: default True
    uid: default True
  uid: bool, default True
- fullName: bool, optional
  name: bool, optional
  spec.python:
  - fullName: bool
    name: bool
    uid: bool
  - fullName: ', '
    name: ', '
  - fullName: optional
    name: optional
    uid: optional
  uid: bool, optional
- fullName: int, default None
  name: int, default None
  spec.python:
  - fullName: int
    name: int
    uid: int
  - fullName: ', '
    name: ', '
  - fullName: default None
    name: default None
    uid: default None
  uid: int, default None
- fullName: optional, int
  name: optional, int
  spec.python:
  - fullName: optional
    name: optional
    uid: optional
  - fullName: ', '
    name: ', '
  - fullName: int
    name: int
    uid: int
  uid: optional, int
- fullName: bool, default
  name: bool, default
  spec.python:
  - fullName: bool
    name: bool
    uid: bool
  - fullName: ', '
    name: ', '
  - fullName: default
    name: default
    uid: default
  uid: bool, default
- fullName: float, list, output of cntk.learners.momentum_schedule
  name: float, list, momentum_schedule
  spec.python:
  - fullName: float
    name: float
    uid: float
  - fullName: ', '
    name: ', '
  - fullName: list
    name: list
    uid: list
  - fullName: ', '
    name: ', '
  - fullName: output of cntk.learners.momentum_schedule
    name: momentum_schedule
    uid: output of cntk.learners.momentum_schedule
  uid: float, list, output of cntk.learners.momentum_schedule
- fullName: float, list, output of cntk.learners.momentum_schedule
  name: float, list, momentum_schedule
  spec.python:
  - fullName: float
    name: float
    uid: float
  - fullName: ', '
    name: ', '
  - fullName: list
    name: list
    uid: list
  - fullName: ', '
    name: ', '
  - fullName: output of cntk.learners.momentum_schedule
    name: momentum_schedule
    uid: output of cntk.learners.momentum_schedule
  uid: float, list, output of cntk.learners.momentum_schedule
