### YamlMime:UniversalReference
api_name: []
items:
- children:
  - cntk.losses.binary_cross_entropy
  - cntk.losses.cosine_distance
  - cntk.losses.cosine_distance_with_negative_samples
  - cntk.losses.cross_entropy_with_softmax
  - cntk.losses.hierarchical_softmax_layer
  - cntk.losses.lambda_rank
  - cntk.losses.lattice_sequence_with_softmax
  - cntk.losses.nce_loss
  - cntk.losses.squared_error
  - cntk.losses.weighted_binary_cross_entropy
  fullName: cntk.losses
  langs:
  - python
  module: cntk.losses
  name: losses
  source:
    id: losses
    path: bindings/python/cntk\losses\__init__.py
    remote:
      branch: release/2.5
      path: bindings/python/cntk\losses\__init__.py
      repo: https://github.com/Microsoft/CNTK.git
    startLine: 0
  summary: 'Loss functions.

    '
  type: package
  uid: cntk.losses
- fullName: cntk.losses.binary_cross_entropy
  langs:
  - python
  module: cntk.losses
  name: binary_cross_entropy
  source:
    id: binary_cross_entropy
    path: bindings/python/cntk\internal\swig_helper.py
    remote:
      branch: release/2.5
      path: bindings/python/cntk\internal\swig_helper.py
      repo: https://github.com/Microsoft/CNTK.git
    startLine: 94
  summary: 'Computes the binary cross entropy (aka logistic loss) between the `output`
    and `target`.



    '
  syntax:
    content: binary_cross_entropy(output, target, name='')
    parameters:
    - description: 'the computed posterior probability for a variable to be 1 from
        the network (typ. a `sigmoid`)

        '
      id: output
    - description: 'ground-truth label, 0 or 1

        '
      id: target
    - description: 'the name of the Function instance in the network

        '
      id: name
      type:
      - str, optional
    return:
      description: '@cntk.ops.functions.Function

        '
  type: function
  uid: cntk.losses.binary_cross_entropy
- example:
  - "\n```\n\n>>> a = np.asarray([-1, -1, -1, 1, 1, -1, 1, 1, -1, 1, 1, -1]).reshape(3,2,2)\n\
    >>> b = np.asarray([1, 1, -1, 1, 1, -1, 1, -1, -1, -1, -1, 1]).reshape(3,2,2)\n\
    >>> x = C.sequence.input_variable(shape=(2,))\n>>> y = C.sequence.input_variable(shape=(2,))\n\
    >>> np.round(C.cosine_distance(x,y).eval({x:a,y:b}),5)\narray([[-1.,  1.],\n \
    \      [ 1.,  0.],\n       [ 0., -1.]], dtype=float32)\n```\n"
  fullName: cntk.losses.cosine_distance
  langs:
  - python
  module: cntk.losses
  name: cosine_distance
  source:
    id: cosine_distance
    path: bindings/python/cntk\internal\swig_helper.py
    remote:
      branch: release/2.5
      path: bindings/python/cntk\internal\swig_helper.py
      repo: https://github.com/Microsoft/CNTK.git
    startLine: 22
  summary: 'Computes the cosine distance between `x` and `y`:

    '
  syntax:
    content: cosine_distance(x, y, name='')
    parameters:
    - description: 'numpy array or any @cntk.ops.functions.Function that outputs a
        tensor

        '
      id: x
    - description: 'the name of the Function instance in the network

        '
      id: name
      type:
      - str, optional
    return:
      description: '@cntk.ops.functions.Function

        '
  type: function
  uid: cntk.losses.cosine_distance
- example:
  - "\n```\n\n>>> qry = np.asarray([1., 1., 0., 0., 0., 1., 1., 0., 0., 0., 1., 1.],\
    \ dtype=np.float32).reshape(3, 1, 4)\n>>> doc = np.asarray([1., 1., 0., 0., 0.,\
    \ 1., 1., 0., 0., 0., 1., 1.], dtype=np.float32).reshape(3, 1, 4)\n>>> x = C.sequence.input_variable(shape=(4,))\n\
    >>> y = C.sequence.input_variable(shape=(4,))\n>>> model = C.cosine_distance_with_negative_samples(x,\
    \ y, shift=1, num_negative_samples=2)\n>>> np.round(model.eval({x: qry, y: doc}),\
    \ decimals=4)\narray([[[ 1. ,  0.5,  0. ]],\n<BLANKLINE>\n       [[ 1. ,  0.5,\
    \  0.5]],\n<BLANKLINE>\n       [[ 1. ,  0. ,  0.5]]], dtype=float32)\n```\n"
  fullName: cntk.losses.cosine_distance_with_negative_samples
  langs:
  - python
  module: cntk.losses
  name: cosine_distance_with_negative_samples
  source:
    id: cosine_distance_with_negative_samples
    path: bindings/python/cntk\internal\swig_helper.py
    remote:
      branch: release/2.5
      path: bindings/python/cntk\internal\swig_helper.py
      repo: https://github.com/Microsoft/CNTK.git
    startLine: 51
  summary: 'Given minibatches for `x` and `y`, this function computes for each element
    in *x* the cosine distance between

    it and the corresponding *y* and additionally the cosine distance between `x`
    and some other elements of `y`

    (referred to a negative samples). The `x` and `y` pairs are samples often derived

    from embeddings of textual data, though the function can be used for any form
    of numeric encodings.

    When using this function to compute textual similarity, `x` represents search
    query term embedding

    and `y` represents a document embedding. The negative samples are formed on the
    fly by shifting

    the right side (`y`). The `shift` indicates how many samples in `y` one should
    shift while

    forming each negative sample pair. It is often chosen to be 1. As the name suggests

    `num_negative_samples` indicates how many negative samples one would want to generate.

    '
  syntax:
    content: cosine_distance_with_negative_samples(x, y, shift, num_negative_samples,
      name='')
    parameters:
    - description: 'numpy array or any @cntk.ops.functions.Function that outputs a
        tensor

        '
      id: x
    - description: 'numpy array or any @cntk.ops.functions.Function that outputs a
        tensor

        '
      id: y
    - description: 'non-zero positive integer representing number of shift to generate
        a negative sample

        '
      id: shift
    - description: 'number of negative samples to generate, a non-zero positive integer

        '
      id: num_negative_samples
    - description: 'the name of the Function instance in the network

        '
      id: name
      type:
      - str, optional
    return:
      description: '@cntk.ops.functions.Function

        '
  type: function
  uid: cntk.losses.cosine_distance_with_negative_samples
- example:
  - '

    ```


    >>> C.cross_entropy_with_softmax([[1., 1., 1., 50.]], [[0., 0., 0., 1.]]).eval()

    array([[ 0.]], dtype=float32)

    ```



    ```


    >>> C.cross_entropy_with_softmax([[1., 2., 3., 4.]], [[0.35, 0.15, 0.05, 0.45]]).eval()

    array([[ 1.84019]], dtype=float32)

    ```

    '
  fullName: cntk.losses.cross_entropy_with_softmax
  langs:
  - python
  module: cntk.losses
  name: cross_entropy_with_softmax
  source:
    id: cross_entropy_with_softmax
    path: bindings/python/cntk\internal\swig_helper.py
    remote:
      branch: release/2.5
      path: bindings/python/cntk\internal\swig_helper.py
      repo: https://github.com/Microsoft/CNTK.git
    startLine: 140
  summary: 'This operation computes the cross entropy between the `target_vector`
    and

    the softmax of the `output_vector`. The elements of `target_vector`

    have to be non-negative and should sum to 1. The `output_vector` can

    contain any values. The function will internally compute the softmax of

    the `output_vector`. Concretely,




    with the understanding that the implementation can use equivalent formulas

    for efficiency and numerical stability.

    '
  syntax:
    content: cross_entropy_with_softmax(output_vector, target_vector, axis=-1, name='')
    parameters:
    - description: 'the unscaled computed output values from the network

        '
      id: output_vector
    - description: 'usually it is one-hot vector where the hot bit

        corresponds to the label index. But it can be any probability

        distribution over the labels.

        '
      id: target_vector
    - description: 'if given, cross entropy will be computed

        along this axis

        '
      id: axis
      type:
      - int
      - cntk.axis.Axis, optional
    - description: 'the name of the Function instance in the network

        '
      id: name
      type:
      - str, optional
    return:
      description: '@cntk.ops.functions.Function

        '
  type: function
  uid: cntk.losses.cross_entropy_with_softmax
- fullName: cntk.losses.hierarchical_softmax_layer
  langs:
  - python
  module: cntk.losses
  name: hierarchical_softmax_layer
  source:
    id: hierarchical_softmax_layer
    path: bindings/python/cntk\internal\swig_helper.py
    remote:
      branch: release/2.5
      path: bindings/python/cntk\internal\swig_helper.py
      repo: https://github.com/Microsoft/CNTK.git
    startLine: 366
  summary: 'A two layers hierarchical softmax function:

    '
  syntax:
    content: hierarchical_softmax_layer(input_var, label_index, label_dim, label_classes=None)
    parameters:
    - description: 'Variable with shape: [#,*](dim_x)

        '
      id: input_var
    - description: 'index of label''s category:  [#,*](1)

        '
      id: label_index
    - description: 'number of the label categories

        '
      id: label_dim
    - description: 'number of classes of the label categories

        '
      id: label_classes
    return:
      description: 'the probability of the given label [#,*](1)

        class_probs: the probability of all the label classes [#,*](label_classes)

        all_probs: the probability of all label classes

        '
      type:
      - output_prob
  type: function
  uid: cntk.losses.hierarchical_softmax_layer
- example:
  - "\n```\n\n>>> group = C.input_variable((1,))\n>>> score = C.input_variable((1,),\
    \ needs_gradient=True)\n>>> gain  = C.input_variable((1,))\n>>> g = np.array([1,\
    \ 1, 2, 2], dtype=np.float32).reshape(4,1)\n>>> s = np.array([1, 2, 3, 4], dtype=np.float32).reshape(4,1)\n\
    >>> n = np.array([7, 1, 3, 1], dtype=np.float32).reshape(4,1)\n>>> f = C.lambda_rank(score,\
    \ gain, group)\n>>> np.round(f.grad({score:s, gain:n, group: g}, wrt=[score]),4)\n\
    array([[-0.2121],\n<BLANKLINE>\n       [ 0.2121],\n<BLANKLINE>\n       [-0.1486],\n\
    <BLANKLINE>\n       [ 0.1486]], dtype=float32)\n```\n"
  fullName: cntk.losses.lambda_rank
  langs:
  - python
  module: cntk.losses
  name: lambda_rank
  source:
    id: lambda_rank
    path: bindings/python/cntk\internal\swig_helper.py
    remote:
      branch: release/2.5
      path: bindings/python/cntk\internal\swig_helper.py
      repo: https://github.com/Microsoft/CNTK.git
    startLine: 212
  summary: 'Groups samples according to `group`, sorts

    them within each group based on `output` and

    computes the Normalized Discounted Cumulative Gain

    (NDCG) at infinity for each group. Concretely,

    the Discounted Cumulative Gain (DCG) at infinity is:



    where  means the gain of the -th ranked sample.


    The NDCG is just the DCG  divided by the maximum achievable DCG (obtained

    by placing the samples with the largest gain at the top of the ranking).


    Samples in the same group must appear in order of decreasing gain.


    It returns 1 minus the average NDCG across all the groups in the minibatch

    multiplied by 100 times the number of samples in the minibatch.


    In the backward direction it back-propagates LambdaRank gradients.

    '
  syntax:
    content: lambda_rank(output, gain, group, name='')
    parameters:
    - description: 'score of each sample

        '
      id: output
    - description: 'gain of each sample

        '
      id: gain
    - description: 'group of each sample

        '
      id: group
    - description: 'the name of the Function instance in the network

        '
      id: name
      type:
      - str, optional
    return:
      description: '@cntk.ops.functions.Function

        '
  type: function
  uid: cntk.losses.lambda_rank
- fullName: cntk.losses.lattice_sequence_with_softmax
  langs:
  - python
  module: cntk.losses
  name: lattice_sequence_with_softmax
  source:
    id: lattice_sequence_with_softmax
    path: bindings/python/cntk\internal\swig_helper.py
    remote:
      branch: release/2.5
      path: bindings/python/cntk\internal\swig_helper.py
      repo: https://github.com/Microsoft/CNTK.git
    startLine: 354
  syntax:
    content: lattice_sequence_with_softmax(label, prediction, loglikelihood, lattice,
      symListPath, phonePath, stateListPath, transProbPath, latticeConfigPath='LatticeNode.config',
      hSmoothingWeight=0.95, frameDropThresh=1e-10, doReferenceAlign=False, seqGammarUsesMBR=False,
      seqGammarAMF=14.0, seqGammarLMF=14.0, seqGammarBMMIFactor=0.0, seqGammarWordPen=0.0,
      name='')
  type: function
  uid: cntk.losses.lattice_sequence_with_softmax
- example:
  - '

    ```


    >>> import scipy

    >>> # dimensions of input, number of noise labels, batch size, number of classes

    >>> xdim = 10

    >>> samples = 32

    >>> batch = 4

    >>> classes = 100

    >>> # some variables; typically x will be the output of a layer

    >>> x = C.input_variable(xdim)

    >>> y = C.input_variable(classes, is_sparse=True)

    >>> # dummy data

    >>> x0 = np.arange(batch * xdim, dtype=np.float32).reshape((batch, xdim))/(batch
    * xdim)

    >>> data = np.ones(batch, dtype=np.float32)

    >>> indices = list(range(10, 10*batch+1, 10))

    >>> indptr = list(range(batch + 1))

    >>> y0 = scipy.sparse.csr_matrix((data, indices, indptr), shape=(batch, classes))

    >>> # a dummy noise distribution

    >>> q = np.arange(classes, dtype=np.float32) + 1 # normalization not necessary

    >>> # the parameters

    >>> b = C.parameter((classes, 1), init=-np.log(classes))

    >>> W = C.parameter((classes, C.InferredDimension), init=C.glorot_uniform(seed=98052))

    >>> # the loss

    >>> loss = C.nce_loss(W, b, x, y, q, seed=98052)

    >>> # evaluate the loss at our dummy data

    >>> np.round(loss.eval({x:x0, y:y0}), decimals=3)

    array([ 2.385,  3.035,  3.886,  3.868], dtype=float32)

    >>> # after training, use the logits for predictions

    >>> logits = C.times_transpose(x, W) + C.reshape(b, -1)

    ```

    '
  fullName: cntk.losses.nce_loss
  langs:
  - python
  module: cntk.losses
  name: nce_loss
  seealsoContent: "See also: [1] C. Dyer. [Notes on Noise Contrastive Estimation and\
    \ Negative Sampling [pdf]](http://demo.clab.cs.cmu.edu/cdyer/nce_notes.pdf). \n"
  source:
    id: nce_loss
    path: bindings/python/cntk\internal\swig_helper.py
    remote:
      branch: release/2.5
      path: bindings/python/cntk\internal\swig_helper.py
      repo: https://github.com/Microsoft/CNTK.git
    startLine: 268
  summary: 'Computes the noise contrastive estimation loss. This implementation mostly

    follows Chris Dyer''s notes [1]. At a high level, this layer draws

    *num_samples* random labels from *noise_distribution* and then forms

    *num_samples`+1 binary classification problems where the true label is

    considered a positive example and the random labels are considered negative

    examples. The negatives are shared among all the examples in the

    minibatch. This operation only computes the logits for the labels in the

    minibatch and the random labels drawn from `noise_distribution*. The

    gradients will be sparse if the labels are sparse.


    The *noise_distribution* is read once and certain quantities are

    precomputed based on it. This operation will need to be reinstantiated if

    the *noise_distribution* changes.


    Shape inference for the weights is currently not supported when inputs are

    placeholders. Either a concrete input must be used or the weights must be

    provided without any inferred dimensions.

    '
  syntax:
    content: nce_loss(weights, biases, inputs, labels, noise_distribution, num_samples=32,
      allow_duplicates=True, seed=auto_select, name='')
    parameters:
    - description: 'parameter (or variable in general) containing the weights with

        which inputs will be multiplied. Its shape must be

        (number of classes, dimension of input)

        '
      id: weights
    - description: 'parameter (or variable in general) containing the biases that

        will be added to the product of weights and inputs. Its shape must be

        (number of classes, 1)

        '
      id: biases
    - description: 'vector of inputs to this layer. Multiplying by the weights and

        adding the biases gives the logits.

        '
      id: inputs
    - description: 'a one-hot vector with the ground-truth labels.

        '
      id: labels
    - description: 'a constant vector with dimension equal to the number

        of classes. The entries must be positive numbers but do not have to

        sum to 1. random labels will be drawn according to the normalized

        distribution.

        '
      id: noise_distribution
    - description: 'number of random labels that will be drawn from the

        *noise_distribution*.

        '
      id: num_samples
    - description: 'boolean. If True (default), the random labels can

        contain duplicates. Compared to *allow_duplicates=False* it is faster

        but the quality of the approximations is slightly worse for the same

        number of samples.

        '
      id: allow_duplicates
    - description: 'random seed. The default value selects a unique random seed.

        '
      id: seed
    - description: 'the name of the Function instance in the network

        '
      id: name
      type:
      - str, optional
    return:
      description: '@cntk.ops.functions.Function

        '
  type: function
  uid: cntk.losses.nce_loss
- example:
  - '

    ```


    >>> i1 = C.input_variable((1,2))

    >>> i2 = C.input_variable((1,2))

    >>> C.squared_error(i1,i2).eval({i1:np.asarray([[[2., 1.]]], dtype=np.float32),
    i2:np.asarray([[[4., 6.]]], dtype=np.float32)})

    array([ 29.], dtype=float32)

    ```



    ```


    >>> C.squared_error(i1,i2).eval({i1:np.asarray([[[1., 2.]]], dtype=np.float32),
    i2:np.asarray([[[1., 2.]]], dtype=np.float32)})

    array([ 0.], dtype=float32)

    ```

    '
  fullName: cntk.losses.squared_error
  langs:
  - python
  module: cntk.losses
  name: squared_error
  source:
    id: squared_error
    path: bindings/python/cntk\internal\swig_helper.py
    remote:
      branch: release/2.5
      path: bindings/python/cntk\internal\swig_helper.py
      repo: https://github.com/Microsoft/CNTK.git
    startLine: 182
  summary: 'This operation computes the sum of the squared difference between elements

    in the two input matrices. The result is a scalar (i.e., one by one matrix).

    This is often used as a training criterion.

    '
  syntax:
    content: squared_error(output, target, name='')
    parameters:
    - description: 'the output values from the network

        '
      id: output
    - description: 'it is usually a one-hot vector where the hot bit

        corresponds to the label index

        '
      id: target
    - description: 'the name of the Function instance in the network

        '
      id: name
      type:
      - str, optional
    return:
      description: '@cntk.ops.functions.Function

        '
  type: function
  uid: cntk.losses.squared_error
- fullName: cntk.losses.weighted_binary_cross_entropy
  langs:
  - python
  module: cntk.losses
  name: weighted_binary_cross_entropy
  source:
    id: weighted_binary_cross_entropy
    path: bindings/python/cntk\internal\swig_helper.py
    remote:
      branch: release/2.5
      path: bindings/python/cntk\internal\swig_helper.py
      repo: https://github.com/Microsoft/CNTK.git
    startLine: 116
  summary: 'This operation computes the weighted binary cross entropy (aka logistic
    loss) between the `output` and `target`.



    '
  syntax:
    content: weighted_binary_cross_entropy(output, target, weight, name='')
    parameters:
    - description: 'the computed posterior probability from the network

        '
      id: output
    - description: 'ground-truth label, 0 or 1

        '
      id: target
    - description: 'weight of each example

        '
      id: weight
    - description: 'the name of the Function instance in the network

        '
      id: name
      type:
      - str, optional
    return:
      description: '@cntk.ops.functions.Function

        '
  type: function
  uid: cntk.losses.weighted_binary_cross_entropy
references:
- fullName: cntk.losses.binary_cross_entropy
  isExternal: false
  name: binary_cross_entropy
  parent: cntk.losses
  uid: cntk.losses.binary_cross_entropy
- fullName: cntk.losses.cosine_distance
  isExternal: false
  name: cosine_distance
  parent: cntk.losses
  uid: cntk.losses.cosine_distance
- fullName: cntk.losses.cosine_distance_with_negative_samples
  isExternal: false
  name: cosine_distance_with_negative_samples
  parent: cntk.losses
  uid: cntk.losses.cosine_distance_with_negative_samples
- fullName: cntk.losses.cross_entropy_with_softmax
  isExternal: false
  name: cross_entropy_with_softmax
  parent: cntk.losses
  uid: cntk.losses.cross_entropy_with_softmax
- fullName: cntk.losses.hierarchical_softmax_layer
  isExternal: false
  name: hierarchical_softmax_layer
  parent: cntk.losses
  uid: cntk.losses.hierarchical_softmax_layer
- fullName: cntk.losses.lambda_rank
  isExternal: false
  name: lambda_rank
  parent: cntk.losses
  uid: cntk.losses.lambda_rank
- fullName: cntk.losses.lattice_sequence_with_softmax
  isExternal: false
  name: lattice_sequence_with_softmax
  parent: cntk.losses
  uid: cntk.losses.lattice_sequence_with_softmax
- fullName: cntk.losses.nce_loss
  isExternal: false
  name: nce_loss
  parent: cntk.losses
  uid: cntk.losses.nce_loss
- fullName: cntk.losses.squared_error
  isExternal: false
  name: squared_error
  parent: cntk.losses
  uid: cntk.losses.squared_error
- fullName: cntk.losses.weighted_binary_cross_entropy
  isExternal: false
  name: weighted_binary_cross_entropy
  parent: cntk.losses
  uid: cntk.losses.weighted_binary_cross_entropy
- fullName: str, optional
  name: str, optional
  spec.python:
  - fullName: str
    name: str
    uid: str
  - fullName: ', '
    name: ', '
  - fullName: optional
    name: optional
    uid: optional
  uid: str, optional
- fullName: cntk.axis.Axis, optional
  name: Axis, optional
  spec.python:
  - fullName: cntk.axis.Axis
    name: Axis
    uid: cntk.axis.Axis
  - fullName: ', '
    name: ', '
  - fullName: optional
    name: optional
    uid: optional
  uid: cntk.axis.Axis, optional
