### YamlMime:UniversalReference
api_name: []
items:
- children:
  - cntk.contrib.deeprl.agent.policy_gradient.ActorCritic.end
  - cntk.contrib.deeprl.agent.policy_gradient.ActorCritic.save
  - cntk.contrib.deeprl.agent.policy_gradient.ActorCritic.save_parameter_settings
  - cntk.contrib.deeprl.agent.policy_gradient.ActorCritic.set_as_best_model
  - cntk.contrib.deeprl.agent.policy_gradient.ActorCritic.start
  - cntk.contrib.deeprl.agent.policy_gradient.ActorCritic.step
  class: cntk.contrib.deeprl.agent.policy_gradient.ActorCritic
  fullName: cntk.contrib.deeprl.agent.policy_gradient.ActorCritic
  inheritance:
  - inheritance:
    - type: builtins.object
    type: cntk.contrib.deeprl.agent.agent.AgentBaseClass
  langs:
  - python
  module: cntk.contrib.deeprl.agent.policy_gradient
  name: ActorCritic
  source:
    id: ActorCritic
    path: bindings/python/cntk\contrib\deeprl\agent\policy_gradient.py
    remote:
      branch: release/2.5
      path: bindings/python/cntk\contrib\deeprl\agent\policy_gradient.py
      repo: https://github.com/Microsoft/CNTK.git
    startLine: 19
  summary: 'Actor-Critic Policy Gradient.


    See [https://arxiv.org/pdf/1602.01783.pdf](https://arxiv.org/pdf/1602.01783.pdf)
    for a description of algorithm.







    '
  syntax:
    content: ActorCritic(config_filename, o_space, a_space)
  type: class
  uid: cntk.contrib.deeprl.agent.policy_gradient.ActorCritic
- class: cntk.contrib.deeprl.agent.policy_gradient.ActorCritic
  fullName: cntk.contrib.deeprl.agent.policy_gradient.ActorCritic.end
  langs:
  - python
  module: cntk.contrib.deeprl.agent.policy_gradient
  name: end
  source:
    id: end
    path: bindings/python/cntk\contrib\deeprl\agent\policy_gradient.py
    remote:
      branch: release/2.5
      path: bindings/python/cntk\contrib\deeprl\agent\policy_gradient.py
      repo: https://github.com/Microsoft/CNTK.git
    startLine: 118
  summary: 'Last observed reward/state of the episode (which then terminates).

    '
  syntax:
    content: end(reward, next_state)
    parameters:
    - description: 'amount of reward returned after previous action.

        '
      id: reward
      type:
      - float
    - description: 'observation provided by the environment.

        '
      id: next_state
      type:
      - object
  type: method
  uid: cntk.contrib.deeprl.agent.policy_gradient.ActorCritic.end
- class: cntk.contrib.deeprl.agent.policy_gradient.ActorCritic
  fullName: cntk.contrib.deeprl.agent.policy_gradient.ActorCritic.save
  langs:
  - python
  module: cntk.contrib.deeprl.agent.policy_gradient
  name: save
  source:
    id: save
    path: bindings/python/cntk\contrib\deeprl\agent\policy_gradient.py
    remote:
      branch: release/2.5
      path: bindings/python/cntk\contrib\deeprl\agent\policy_gradient.py
      repo: https://github.com/Microsoft/CNTK.git
    startLine: 276
  summary: 'Save model to file.

    '
  syntax:
    content: save(filename)
    parameters:
    - id: self
    - id: filename
  type: method
  uid: cntk.contrib.deeprl.agent.policy_gradient.ActorCritic.save
- class: cntk.contrib.deeprl.agent.policy_gradient.ActorCritic
  fullName: cntk.contrib.deeprl.agent.policy_gradient.ActorCritic.save_parameter_settings
  langs:
  - python
  module: cntk.contrib.deeprl.agent.policy_gradient
  name: save_parameter_settings
  source:
    id: save_parameter_settings
    path: bindings/python/cntk\contrib\deeprl\agent\policy_gradient.py
    remote:
      branch: release/2.5
      path: bindings/python/cntk\contrib\deeprl\agent\policy_gradient.py
      repo: https://github.com/Microsoft/CNTK.git
    startLine: 280
  summary: 'Save parameter settings to file.

    '
  syntax:
    content: save_parameter_settings(filename)
    parameters:
    - id: self
    - id: filename
  type: method
  uid: cntk.contrib.deeprl.agent.policy_gradient.ActorCritic.save_parameter_settings
- class: cntk.contrib.deeprl.agent.policy_gradient.ActorCritic
  fullName: cntk.contrib.deeprl.agent.policy_gradient.ActorCritic.set_as_best_model
  langs:
  - python
  module: cntk.contrib.deeprl.agent.policy_gradient
  name: set_as_best_model
  source:
    id: set_as_best_model
    path: bindings/python/cntk\contrib\deeprl\agent\policy_gradient.py
    remote:
      branch: release/2.5
      path: bindings/python/cntk\contrib\deeprl\agent\policy_gradient.py
      repo: https://github.com/Microsoft/CNTK.git
    startLine: 134
  summary: 'Copy current model to best model.

    '
  syntax:
    content: set_as_best_model()
    parameters:
    - id: self
  type: method
  uid: cntk.contrib.deeprl.agent.policy_gradient.ActorCritic.set_as_best_model
- class: cntk.contrib.deeprl.agent.policy_gradient.ActorCritic
  fullName: cntk.contrib.deeprl.agent.policy_gradient.ActorCritic.start
  langs:
  - python
  module: cntk.contrib.deeprl.agent.policy_gradient
  name: start
  source:
    id: start
    path: bindings/python/cntk\contrib\deeprl\agent\policy_gradient.py
    remote:
      branch: release/2.5
      path: bindings/python/cntk\contrib\deeprl\agent\policy_gradient.py
      repo: https://github.com/Microsoft/CNTK.git
    startLine: 63
  summary: 'Start a new episode.

    '
  syntax:
    content: start(state)
    parameters:
    - description: 'observation provided by the environment.

        '
      id: state
      type:
      - object
    return:
      description: 'action choosen by agent.

        debug_info (dict): auxiliary diagnostic information.

        '
      type:
      - action (int)
  type: method
  uid: cntk.contrib.deeprl.agent.policy_gradient.ActorCritic.start
- class: cntk.contrib.deeprl.agent.policy_gradient.ActorCritic
  fullName: cntk.contrib.deeprl.agent.policy_gradient.ActorCritic.step
  langs:
  - python
  module: cntk.contrib.deeprl.agent.policy_gradient
  name: step
  source:
    id: step
    path: bindings/python/cntk\contrib\deeprl\agent\policy_gradient.py
    remote:
      branch: release/2.5
      path: bindings/python/cntk\contrib\deeprl\agent\policy_gradient.py
      repo: https://github.com/Microsoft/CNTK.git
    startLine: 92
  summary: 'Observe one transition and choose an action.

    '
  syntax:
    content: step(reward, next_state)
    parameters:
    - description: 'amount of reward returned after previous action.

        '
      id: reward
      type:
      - float
    - description: 'observation provided by the environment.

        '
      id: next_state
      type:
      - object
    return:
      description: 'action choosen by agent.

        debug_info (dict): auxiliary diagnostic information.

        '
      type:
      - action (int)
  type: method
  uid: cntk.contrib.deeprl.agent.policy_gradient.ActorCritic.step
references:
- fullName: cntk.contrib.deeprl.agent.policy_gradient.ActorCritic.end
  isExternal: false
  name: end
  parent: cntk.contrib.deeprl.agent.policy_gradient.ActorCritic
  uid: cntk.contrib.deeprl.agent.policy_gradient.ActorCritic.end
- fullName: cntk.contrib.deeprl.agent.policy_gradient.ActorCritic.save
  isExternal: false
  name: save
  parent: cntk.contrib.deeprl.agent.policy_gradient.ActorCritic
  uid: cntk.contrib.deeprl.agent.policy_gradient.ActorCritic.save
- fullName: cntk.contrib.deeprl.agent.policy_gradient.ActorCritic.save_parameter_settings
  isExternal: false
  name: save_parameter_settings
  parent: cntk.contrib.deeprl.agent.policy_gradient.ActorCritic
  uid: cntk.contrib.deeprl.agent.policy_gradient.ActorCritic.save_parameter_settings
- fullName: cntk.contrib.deeprl.agent.policy_gradient.ActorCritic.set_as_best_model
  isExternal: false
  name: set_as_best_model
  parent: cntk.contrib.deeprl.agent.policy_gradient.ActorCritic
  uid: cntk.contrib.deeprl.agent.policy_gradient.ActorCritic.set_as_best_model
- fullName: cntk.contrib.deeprl.agent.policy_gradient.ActorCritic.start
  isExternal: false
  name: start
  parent: cntk.contrib.deeprl.agent.policy_gradient.ActorCritic
  uid: cntk.contrib.deeprl.agent.policy_gradient.ActorCritic.start
- fullName: cntk.contrib.deeprl.agent.policy_gradient.ActorCritic.step
  isExternal: false
  name: step
  parent: cntk.contrib.deeprl.agent.policy_gradient.ActorCritic
  uid: cntk.contrib.deeprl.agent.policy_gradient.ActorCritic.step
- fullName: action (int)
  name: action (int)
  spec.python:
  - fullName: 'action '
    name: 'action '
    uid: 'action '
  - fullName: (
    name: (
  - fullName: int
    name: int
    uid: int
  - fullName: )
    name: )
  uid: action (int)
